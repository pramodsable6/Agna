{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f2c3b8-3c28-441f-873d-ed019111f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, TimestampType, DateType, FloatType\n",
    "from pyspark.sql.functions import sum, col, count, when, lead, collect_list, upper, lower,\\\n",
    "    length, year, countDistinct, round, first, lit, dense_rank, substring, udf, broadcast, skewness\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc3c92b-eb15-49f9-8b16-3e3939c81c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/29 17:11:24 WARN Utils: Your hostname, Pramod resolves to a loopback address: 127.0.1.1; using 172.23.91.132 instead (on interface eth0)\n",
      "23/09/29 17:11:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/29 17:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\n",
    "    'local[*]').appName('PysparkDataPipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac165f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/30 23:56:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "# from delta import *\n",
    "\n",
    "# builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74311c29-9756-4d65-9cc8-3d8e101390fa",
   "metadata": {},
   "source": [
    "#### Creating an empty Df -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb01bdc-c8b3-498d-8a14-4d061a3c1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('column_a', IntegerType(), True),\n",
    "                    StructField('column_b', StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff5845b-115b-4fd1-9f6e-6e6f69974f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6481333-129d-4bd0-971f-c8689be62dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cc575-9618-4b57-8d2a-b3f76344ea00",
   "metadata": {},
   "source": [
    "#### Append Data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cacccdd-6f39-4dda-9be4-8112a335a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([[1, 'a'],\n",
    "                             [2, 'b'],\n",
    "                             [3, 'c'],\n",
    "                             [None, 'd']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "11a5bb01-1f41-4e26-b7d3-65b3b8a7b330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|    null|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c9b5417-5d78-4e17-be08-7a56427efdfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96316272-0ac4-4c06-a6c0-169901f316a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|    null|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea79d3-673e-4960-a54f-509b021902b7",
   "metadata": {},
   "source": [
    "#### Fill Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2a0842d-16dc-4950-8a55-d0d7836095a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(1, subset=['column_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18caad25-c871-4232-99da-8c9aae9c66f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|       1|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036857b-81ed-4842-8530-7a768e547fa1",
   "metadata": {},
   "source": [
    "#### Replace all 1's with 2's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f94be0cf-d32a-480e-9070-128b3c8d4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       2|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|       2|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df = df.replace(1, 2, subset=['column_a'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcf1a6-5131-446d-a8a0-a71d1167ecce",
   "metadata": {},
   "source": [
    "#### LeetCode SQL problem\n",
    "#### Replace Employee ID With The Unique Identifier\n",
    "https://leetcode.com/problems/replace-employee-id-with-the-unique-identifier/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e660967-6afb-44b4-8948-2fbe9533bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "employees_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                               StructField('name', StringType(), True)])\n",
    "employees_df = spark.createDataFrame([[1, 'Alice'],\n",
    "                                      [7, 'Bob'],\n",
    "                                      [11, 'Meir'],\n",
    "                                      [90, 'Winston'],\n",
    "                                      [3, 'Jonathan']], schema=employees_schema)\n",
    "\n",
    "employee_uni_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                                  StructField('unique_id', IntegerType(), True)])\n",
    "employee_uni_df = spark.createDataFrame([[3, 1],\n",
    "                                         [11, 2],\n",
    "                                         [90, 3]], schema=employee_uni_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2fc842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|   Alice|\n",
      "|  7|     Bob|\n",
      "| 11|    Meir|\n",
      "| 90| Winston|\n",
      "|  3|Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0105940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|unique_id|\n",
      "+---+---------+\n",
      "|  3|        1|\n",
      "| 11|        2|\n",
      "| 90|        3|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_uni_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c095e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|     null|     Bob|\n",
      "|        2|    Meir|\n",
      "|        1|Jonathan|\n",
      "|        3| Winston|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employee_uni_df, how='left', on=[\n",
    "                  'id']).select('unique_id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca0e3a",
   "metadata": {},
   "source": [
    "#### Leetcode SQL problem - Confirmation Rate\n",
    "https://leetcode.com/problems/confirmation-rate/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f018350",
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_schema = StructType([StructField('user_id', IntegerType(), False),\n",
    "                             StructField('time_stamp', TimestampType(), True)])\n",
    "confirmations_schema = StructType([StructField('user_id', IntegerType(), True),\n",
    "                                   StructField(\n",
    "                                       'time_stamp', TimestampType(), True),\n",
    "                                   StructField('action', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ff7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinups_df = spark.createDataFrame([[3, datetime.strptime('2020-03-21 10:16:13', '%Y-%m-%d %H:%M:%S')],\n",
    "                                   [7, datetime.strptime(\n",
    "                                       '2020-01-04 13:57:59', '%Y-%m-%d %H:%M:%S')],\n",
    "                                   [2, datetime.strptime(\n",
    "                                       '2020-07-29 23:09:44', '%Y-%m-%d %H:%M:%S')],\n",
    "                                   [6, datetime.strptime('2020-12-09 10:39:37', '%Y-%m-%d %H:%M:%S')]], schema=signups_schema)\n",
    "confirmations_df = spark.createDataFrame([[3, datetime.strptime('2021-01-06 03:30:46', '%Y-%m-%d %H:%M:%S'), 'timeout'],\n",
    "                                          [3, datetime.strptime(\n",
    "                                              '2021-07-14 14:00:00', '%Y-%m-%d %H:%M:%S'), 'timeout'],\n",
    "                                          [7, datetime.strptime(\n",
    "                                              '2021-06-12 11:57:29', '%Y-%m-%d %H:%M:%S'), 'confirmed'],\n",
    "                                          [7, datetime.strptime(\n",
    "                                              '2021-06-13 12:58:28', '%Y-%m-%d %H:%M:%S'), 'confirmed'],\n",
    "                                          [7, datetime.strptime(\n",
    "                                              '2021-06-14 13:59:27', '%Y-%m-%d %H:%M:%S'), 'confirmed'],\n",
    "                                          [2, datetime.strptime(\n",
    "                                              '2021-01-22 00:00:00', '%Y-%m-%d %H:%M:%S'), 'confirmed'],\n",
    "                                          [2, datetime.strptime('2021-02-28 23:59:59', '%Y-%m-%d %H:%M:%S'), 'timeout']], schema=confirmations_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52820080",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = sinups_df.join(confirmations_df, how='left', on=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81f5df31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      3|              0.0|\n",
      "|      7|              1.0|\n",
      "|      2|              0.5|\n",
      "|      6|              0.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.groupBy('user_id').agg((sum(when(result_df.action == 'confirmed', 1).otherwise(\n",
    "    0.00)) / count('*')).alias('confirmation_rate')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717588e0",
   "metadata": {},
   "source": [
    "#### Find Users With Valid E-Mails\n",
    "##### https://leetcode.com/problems/find-users-with-valid-e-mails/description/?envType=study-plan-v2&envId=30-days-of-pandas&lang=pythondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb09434",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('user_id', IntegerType(), False),\n",
    "                     StructField('name', StringType(), True),\n",
    "                     StructField('mail', StringType(), True)])\n",
    "df = spark.createDataFrame([[1, 'Winston', 'winston@leetcode.com'],\n",
    "                            [2, 'Jonathan', 'jonathanisgreat'],\n",
    "                            [3, 'Annabelle', 'bella-@leetcode.com'],\n",
    "                            [4, 'Sally', 'sally.come@leetcode.com'],\n",
    "                            [5, 'Marwan', 'quarz#2020@leetcode.com'],\n",
    "                            [6, 'David', 'david69@gmail.com'],\n",
    "                            [7, 'Shapiro', '.shapo@leetcode.com']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b6987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      2| Jonathan|     jonathanisgreat|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "|      5|   Marwan|quarz#2020@leetco...|\n",
      "|      6|    David|   david69@gmail.com|\n",
      "|      7|  Shapiro| .shapo@leetcode.com|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d654e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('mail').rlike('^[a-zA-Z][a-zA-Z0-9_.-]*@leetcode\\.com')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4ea04",
   "metadata": {},
   "source": [
    "#### Consecutive Numbers\n",
    "#### https://leetcode.com/problems/consecutive-numbers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "983086c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('id', IntegerType(), False),\n",
    "                    StructField('num', StringType(), True)])\n",
    "df = spark.createDataFrame([[1, '1'],\n",
    "                            [2, '1'],\n",
    "                            [3, '1'],\n",
    "                            [4, '2'],\n",
    "                            [5, '1'],\n",
    "                            [6, '2'],\n",
    "                            [7, '2']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e634b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d381c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 15:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''with temp as (\n",
    "                select num,\n",
    "                    lead(num, 1) over (order by id) num1,\n",
    "                    lead(num, 2) over (order by id) num2 \n",
    "                from df\n",
    "                )\n",
    "                select distinct num \n",
    "                from temp\n",
    "                where num = num1\n",
    "                and num = num2''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cece067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('num1', lead('num', 1).over(Window.orderBy('id')))\\\n",
    "    .withColumn('num2', lead('num', 2).over(Window.orderBy('id')))\\\n",
    "    .filter((col('num') == col('num1')) & (col('num') == col('num2')))\\\n",
    "    .select('id').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c528fae",
   "metadata": {},
   "source": [
    "### Students and classes example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0721ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('class', IntegerType(), True),\n",
    "                    StructField('student_id', IntegerType(), True),\n",
    "                    StructField('term', IntegerType(), True),\n",
    "                    StructField('subject', StringType(), True),\n",
    "                    StructField('marks', IntegerType(), True)])\n",
    "\n",
    "df = spark.createDataFrame([[2, 1, 1, 'maths', 10],\n",
    "                            [2, 1, 2, 'maths', 12],\n",
    "                            [2, 1, 1, 'english', 14],\n",
    "                            [2, 1, 2, 'english', 12],\n",
    "                            [3, 2, 1, 'maths', 10],\n",
    "                            [3, 2, 2, 'maths', 12],\n",
    "                            [3, 2, 1, 'english', 16],\n",
    "                            [3, 2, 2, 'english', 14]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b4b8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+-------+-----+\n",
      "|class|student_id|term|subject|marks|\n",
      "+-----+----------+----+-------+-----+\n",
      "|    2|         1|   1|  maths|   10|\n",
      "|    2|         1|   2|  maths|   12|\n",
      "|    2|         1|   1|english|   14|\n",
      "|    2|         1|   2|english|   12|\n",
      "|    3|         2|   1|  maths|   10|\n",
      "|    3|         2|   2|  maths|   12|\n",
      "|    3|         2|   1|english|   16|\n",
      "|    3|         2|   2|english|   14|\n",
      "+-----+----------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54aa1c",
   "metadata": {},
   "source": [
    "> Get class 2 students in following format --> class student_id subject term1 term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8360cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy('class', 'student_id', 'subject')\\\n",
    "    .groupBy('class', 'student_id', 'subject')\\\n",
    "    .agg(collect_list('marks')[0].alias('term1'), collect_list('marks')[1].alias('term2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "827e416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+-----+-----+\n",
      "|class|student_id|subject|term1|term2|\n",
      "+-----+----------+-------+-----+-----+\n",
      "|    2|         1|english|   14|   12|\n",
      "|    2|         1|  maths|   10|   12|\n",
      "|    3|         2|english|   16|   14|\n",
      "|    3|         2|  maths|   10|   12|\n",
      "+-----+----------+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1fb6f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+-----+-----+\n",
      "|class|student_id|subject|term1|term2|\n",
      "+-----+----------+-------+-----+-----+\n",
      "|    2|         1|english|   14|   12|\n",
      "|    2|         1|  maths|   10|   12|\n",
      "+-----+----------+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('class') == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0307df",
   "metadata": {},
   "source": [
    "> Get subject-wise aggregated score with 25% weightage to term1 and 75% weightage to term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4170e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[2, 1, 1, 'maths', 10],\n",
    "                            [2, 1, 2, 'maths', 12],\n",
    "                            [2, 1, 1, 'english', 14],\n",
    "                            [2, 1, 2, 'english', 12],\n",
    "                            [3, 2, 1, 'maths', 10],\n",
    "                            [3, 2, 2, 'maths', 12],\n",
    "                            [3, 2, 1, 'english', 16],\n",
    "                            [3, 2, 2, 'english', 14]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a16ac4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+-------+-----+\n",
      "|class|student_id|term|subject|marks|\n",
      "+-----+----------+----+-------+-----+\n",
      "|    2|         1|   1|  maths|   10|\n",
      "|    2|         1|   2|  maths|   12|\n",
      "|    2|         1|   1|english|   14|\n",
      "|    2|         1|   2|english|   12|\n",
      "|    3|         2|   1|  maths|   10|\n",
      "|    3|         2|   2|  maths|   12|\n",
      "|    3|         2|   1|english|   16|\n",
      "|    3|         2|   2|english|   14|\n",
      "+-----+----------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "552d2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "maths_agg = \\\n",
    "    df.filter(col('subject') == 'maths')\\\n",
    "    .orderBy('student_id', 'term')\\\n",
    "    .groupBy('student_id')\\\n",
    "    .agg((collect_list('marks')[0] * 0.25 + collect_list('marks')[1] * 0.75).alias('maths_agg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ec5379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_agg = \\\n",
    "    df.filter(col('subject') == 'english')\\\n",
    "    .orderBy('student_id', 'term')\\\n",
    "    .groupBy('student_id')\\\n",
    "    .agg((collect_list('marks')[0] * 0.25 + collect_list('marks')[1] * 0.75).alias('english_agg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c847409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|student_id|maths_agg|english_agg|\n",
      "+----------+---------+-----------+\n",
      "|         1|     11.5|       12.5|\n",
      "|         2|     11.5|       14.5|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maths_agg.join(english_agg, how='inner', on='student_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb24be7",
   "metadata": {},
   "source": [
    "#### Exchange Seats -\n",
    "#### https://leetcode.com/problems/exchange-seats/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64a42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "data = '''\n",
    "id,student\n",
    "1,Abbot\n",
    "2,Doris\n",
    "3,Emerson\n",
    "4,Green\n",
    "5,Jeames\n",
    "'''\n",
    "df = spark.createDataFrame(pd.read_csv(io.StringIO(data), header=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1317c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Abbot|\n",
      "|  2|  Doris|\n",
      "|  3|Emerson|\n",
      "|  4|  Green|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "383d983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|Student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "max_id = max(df.select('id').collect())[0]\n",
    "df.select(when(col('id') % 2 == 1,\n",
    "               lead('id', 1, max_id).over(Window.orderBy('id')))\n",
    "          .otherwise(col('id') - 1).alias('id'),\n",
    "          'Student').orderBy('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e6135",
   "metadata": {},
   "source": [
    "#### Tree Node\n",
    "#### https://leetcode.com/problems/tree-node/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22009a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('id', IntegerType(), False),\n",
    "                    StructField('p_id', IntegerType(), True)])\n",
    "df = spark.createDataFrame(\n",
    "    [[1, None],\n",
    "     [2, 1],\n",
    "     [3, 1],\n",
    "     [4, 2],\n",
    "     [5, 2]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1587b091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|p_id|\n",
      "+---+----+\n",
      "|  1|null|\n",
      "|  2|   1|\n",
      "|  3|   1|\n",
      "|  4|   2|\n",
      "|  5|   2|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9854cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| type|\n",
      "+---+-----+\n",
      "|  1| Root|\n",
      "|  2|Inner|\n",
      "|  3| Leaf|\n",
      "|  4| Leaf|\n",
      "|  5| Leaf|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_parent_ids = df.select(\n",
    "    'p_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "df.select('id',\n",
    "          when(col('p_id').isNull(), 'Root')\n",
    "          .when(col('p_id').isNotNull() & col('id').isin(distinct_parent_ids), 'Inner')\n",
    "          .otherwise('Leaf').alias('type')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a7b23",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2be51b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69ed1df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>p_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  p_id\n",
       "0   1   NaN\n",
       "1   2   1.0\n",
       "2   3   1.0\n",
       "3   4   2.0\n",
       "4   5   2.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cad500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pdf['type'] = np.where(pdf['p_id'].isna(),\n",
    "                       'Root',\n",
    "                       np.where(pdf['id'].isin(pdf['p_id'].unique()) & pdf['p_id'].notna(),\n",
    "                                'Inner',\n",
    "                                'Leaf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "686d311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Inner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Leaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Leaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Leaf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   type\n",
       "0   1   Root\n",
       "1   2  Inner\n",
       "2   3   Leaf\n",
       "3   4   Leaf\n",
       "4   5   Leaf"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[['id', 'type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342b00d",
   "metadata": {},
   "source": [
    "#### Customers who bought all products\n",
    "#### https://leetcode.com/problems/customers-who-bought-all-products/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c52084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([StructField('customer_id', IntegerType(), False),\n",
    "                              StructField('product_key', IntegerType(), False)])\n",
    "product_schema = StructType([StructField('product_key', IntegerType(), False)])\n",
    "customer_df = spark.createDataFrame([[1, 5],\n",
    "                                     [2, 6],\n",
    "                                     [3, 5],\n",
    "                                     [3, 6],\n",
    "                                     [1, 6]], schema=customer_schema)\n",
    "product_df = spark.createDataFrame([[5], [6]], schema=product_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d5e7140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|product_key|\n",
      "+-----------+-----------+\n",
      "|          1|          5|\n",
      "|          2|          6|\n",
      "|          3|          5|\n",
      "|          3|          6|\n",
      "|          1|          6|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1ba8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|product_key|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          6|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "447f4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = customer_df.dropDuplicates().groupBy(\n",
    "    'customer_id').agg(count(col('customer_id')).alias('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9a6df6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|          1|    2|\n",
      "|          3|    2|\n",
      "|          2|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b823dff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df.filter(col('count') == product_df.count()\n",
    "                  ).select('customer_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707fa508",
   "metadata": {},
   "source": [
    "#### Triangle Judgement\n",
    "#### https://leetcode.com/problems/triangle-judgement/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65836b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('x', IntegerType(), False),\n",
    "                     StructField('y', IntegerType(), False),\n",
    "                     StructField('z', IntegerType(), False)])\n",
    "df = spark.createDataFrame([[13, 15, 30],\n",
    "                            [10, 20, 15]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18064508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  x|  y|  z|\n",
      "+---+---+---+\n",
      "| 13| 15| 30|\n",
      "| 10| 20| 15|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c491f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n",
      "|  x|  y|  z|triangle|\n",
      "+---+---+---+--------+\n",
      "| 13| 15| 30|      No|\n",
      "| 10| 20| 15|     Yes|\n",
      "+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('triangle', when((col('x') + col('y') > col('z'))\n",
    "                               & (col('y') + col('z') > col('x'))\n",
    "                               & (col('x') + col('z') > col('y')), 'Yes')\n",
    "              .otherwise('No')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5fca61",
   "metadata": {},
   "source": [
    "#### Invalid Tweets\n",
    "#### https://leetcode.com/problems/invalid-tweets/?envType=study-plan-v2&envId=30-days-of-pandas&lang=pythondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db55cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('tweet_id', IntegerType(), False),\n",
    "                     StructField('content', StringType(), True)])\n",
    "df = spark.createDataFrame([[1, 'Vote for Biden'],\n",
    "                            [2, 'Let us make America great again!']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d327630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+\n",
      "|tweet_id|content                         |\n",
      "+--------+--------------------------------+\n",
      "|1       |Vote for Biden                  |\n",
      "|2       |Let us make America great again!|\n",
      "+--------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca8d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(length(col('content')) > 15).select('tweet_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988dd7a",
   "metadata": {},
   "source": [
    "#### Calculate Special Bonus\n",
    "#### https://leetcode.com/problems/calculate-special-bonus/description/?envType=study-plan-v2&envId=30-days-of-pandas&lang=pythondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b115ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('employee_id', IntegerType(), False),\n",
    "                     StructField('name', StringType(), True),\n",
    "                     StructField('salary', IntegerType(), True)])\n",
    "df = spark.createDataFrame([[2, 'Meir', 3000],\n",
    "                            [3, 'Michael', 3800],\n",
    "                            [7, 'Addilyn', 7400],\n",
    "                            [8, 'Juan', 6100],\n",
    "                            [9, 'Kannon', 7700]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb4769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+\n",
      "|employee_id|name   |salary|\n",
      "+-----------+-------+------+\n",
      "|2          |Meir   |3000  |\n",
      "|3          |Michael|3800  |\n",
      "|7          |Addilyn|7400  |\n",
      "|8          |Juan   |6100  |\n",
      "|9          |Kannon |7700  |\n",
      "+-----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38877b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|employee_id|bonus|\n",
      "+-----------+-----+\n",
      "|          2|    0|\n",
      "|          3|    0|\n",
      "|          7| 7400|\n",
      "|          8|    0|\n",
      "|          9| 7700|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('bonus', when((col('employee_id') % 2 == 1) & (~col('name').startswith(\n",
    "    'M')), col('salary')).otherwise(0).alias('bonus')).select('employee_id', 'bonus').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661b93e",
   "metadata": {},
   "source": [
    "#### Market Analysis I\n",
    "#### https://leetcode.com/problems/market-analysis-i/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e70ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = StructType([StructField('user_id', IntegerType(), False),\n",
    "                           StructField('join_date', StringType(), False),\n",
    "                           StructField('favorite_brand', StringType(), False)])\n",
    "users_df = spark.createDataFrame([[1, '2018-01-01', 'Lenovo'],\n",
    "                                  [2, '2018-02-09', 'Samsung'],\n",
    "                                  [3, '2018-01-09', 'LG'],\n",
    "                                  [4, '2018-05-21', 'HP']], schema=users_schema)\n",
    "\n",
    "orders_schema = StructType([StructField('order_id', IntegerType(), False),\n",
    "                            StructField('order_date', StringType(), False),\n",
    "                            StructField('item_id', IntegerType(), False),\n",
    "                            StructField('buyer_id', IntegerType(), False),\n",
    "                            StructField('seller_id', IntegerType(), False)])\n",
    "orders_df = spark.createDataFrame([[1, '2019-08-01', 4, 1, 2],\n",
    "                                   [2, '2018-08-02', 2, 1, 3],\n",
    "                                   [3, '2019-08-03', 3, 2, 3],\n",
    "                                   [4, '2018-08-04', 1, 4, 2],\n",
    "                                   [5, '2018-08-04', 1, 3, 4],\n",
    "                                   [6, '2019-08-05', 2, 2, 4]], schema=orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ed6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = users_df.withColumn('join_date', col('join_date').cast('date'))\n",
    "orders_df = orders_df.withColumn('order_date', col('order_date').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af74a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = false)\n",
      " |-- join_date: date (nullable = true)\n",
      " |-- favorite_brand: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29cfd85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- item_id: integer (nullable = false)\n",
      " |-- buyer_id: integer (nullable = false)\n",
      " |-- seller_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4daf193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|buyer_id|orders_in_2019|\n",
      "+--------+--------------+\n",
      "|       1|             1|\n",
      "|       2|             2|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_temp = orders_df.filter(year(col('order_date')) == 2019).groupBy(\n",
    "    col('buyer_id')).agg(count('order_id').alias('orders_in_2019'))\n",
    "orders_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bb5b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+\n",
      "|buyer_id| join_date|orders_in_2019|\n",
      "+--------+----------+--------------+\n",
      "|       1|2018-01-01|             1|\n",
      "|       2|2018-02-09|             2|\n",
      "|       3|2018-01-09|             0|\n",
      "|       4|2018-05-21|             0|\n",
      "+--------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.join(orders_temp, users_df.user_id == orders_temp.buyer_id, how='left')\\\n",
    "    .select(col('user_id').alias('buyer_id'), 'join_date', 'orders_in_2019')\\\n",
    "    .fillna(0, subset=['orders_in_2019']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80767ae",
   "metadata": {},
   "source": [
    "#### Department Highest Salary\n",
    "#### https://leetcode.com/problems/department-highest-salary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2fb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employee_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                              StructField('name', StringType(), False),\n",
    "                              StructField('salary', IntegerType(), False),\n",
    "                              StructField('departmentId', IntegerType(), False)])\n",
    "employee_df = spark.createDataFrame([[1, 'Joe', 70000, 1],\n",
    "                                    [2, 'Jim', 90000, 1],\n",
    "                                    [3, 'Henry', 80000, 2],\n",
    "                                    [4, 'Sam', 60000, 2],\n",
    "                                    [5, 'Max', 90000, 1]], schema=employee_schema)\n",
    "\n",
    "department_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                                StructField('name', StringType(), False)])\n",
    "department_df = spark.createDataFrame([[1, 'IT'],\n",
    "                                       [2, 'Sales']], schema=department_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5c4ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+\n",
      "| id| name|salary|departmentId|\n",
      "+---+-----+------+------------+\n",
      "|  1|  Joe| 70000|           1|\n",
      "|  2|  Jim| 90000|           1|\n",
      "|  3|Henry| 80000|           2|\n",
      "|  4|  Sam| 60000|           2|\n",
      "|  5|  Max| 90000|           1|\n",
      "+---+-----+------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000e161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|   IT|\n",
      "|  2|Sales|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7289cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|departmentId|max_salary|\n",
      "+------------+----------+\n",
      "|           1|     90000|\n",
      "|           2|     80000|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_salary = employee_df.groupby(col('departmentId')).max(\n",
    "    'salary').withColumnRenamed('max(salary)', 'max_salary')\n",
    "max_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe30acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n",
      "|Department|Employee|Salary|\n",
      "+----------+--------+------+\n",
      "|IT        |Max     |90000 |\n",
      "|IT        |Jim     |90000 |\n",
      "|Sales     |Henry   |80000 |\n",
      "+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_salary = max_salary.withColumnRenamed('departmentId', 'depId')\n",
    "employee_df = employee_df.withColumnRenamed('name', 'empName')\n",
    "employee_df.join(max_salary,\n",
    "                 (employee_df['departmentId'] == max_salary['depId'])\n",
    "                 & (employee_df['salary'] == max_salary['max_salary']),\n",
    "                 how='inner')\\\n",
    "    .join(department_df,\n",
    "          employee_df['departmentId'] == department_df['id'],\n",
    "          how='inner')\\\n",
    "    .select(col('name').alias('Department'), col('empName').alias('Employee'), col('salary').alias('Salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61b083",
   "metadata": {},
   "source": [
    "#### The Number of Rich Customers\n",
    "#### https://leetcode.com/problems/the-number-of-rich-customers/description/?envType=study-plan-v2&envId=30-days-of-pandas&lang=pythondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7312f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cd2569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(io.StringIO('''\n",
    "bill_id,customer_id,amount\n",
    "6,1,549\n",
    "8,1,834\n",
    "4,2,394\n",
    "11,3,657\n",
    "13,3,257\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "102125f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_id  customer_id  amount\n",
       "0        6            1     549\n",
       "1        8            1     834\n",
       "2        4            2     394\n",
       "3       11            3     657\n",
       "4       13            3     257"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b26b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8f80241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+\n",
      "|bill_id|customer_id|amount|\n",
      "+-------+-----------+------+\n",
      "|      6|          1|   549|\n",
      "|      8|          1|   834|\n",
      "|      4|          2|   394|\n",
      "|     11|          3|   657|\n",
      "|     13|          3|   257|\n",
      "+-------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bea0b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|rich_count|\n",
      "+----------+\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('amount') > 500).select(countDistinct(\n",
    "    col('customer_id')).alias('rich_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59577fa9",
   "metadata": {},
   "source": [
    "#### Project Employees I\n",
    "#### https://leetcode.com/problems/project-employees-i/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "440bce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_schema = StructType([StructField('project_id', IntegerType(), False),\n",
    "                            StructField('employee_id', IntegerType(), True)])\n",
    "project_df = spark.createDataFrame([[1, 1],\n",
    "                                    [1, 2],\n",
    "                                    [1, 3],\n",
    "                                    [2, 1],\n",
    "                                    [2, 4]], schema=project_schema)\n",
    "\n",
    "employee_schema = StructType([StructField('employee_id', IntegerType(), False),\n",
    "                              StructField('name', StringType(), False),\n",
    "                              StructField('experience_years', IntegerType(), True)])\n",
    "employee_df = spark.createDataFrame([[1, 'Khaled', 3],\n",
    "                                     [2, 'Ali', 2],\n",
    "                                     [3, 'John', 1],\n",
    "                                     [4, 'Doe', 2]], schema=employee_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01d5cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|         1|          1|\n",
      "|         1|          2|\n",
      "|         1|          3|\n",
      "|         2|          1|\n",
      "|         2|          4|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbe68e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------------+\n",
      "|employee_id|  name|experience_years|\n",
      "+-----------+------+----------------+\n",
      "|          1|Khaled|               3|\n",
      "|          2|   Ali|               2|\n",
      "|          3|  John|               1|\n",
      "|          4|   Doe|               2|\n",
      "+-----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d5fbc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|project_id|average_years|\n",
      "+----------+-------------+\n",
      "|         1|          2.0|\n",
      "|         2|          2.5|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.join(employee_df, on='employee_id', how='inner')\\\n",
    "    .groupBy('project_id').avg('experience_years')\\\n",
    "    .select(col('project_id'), round(col('avg(experience_years)'), 2).alias('average_years')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73737b1b",
   "metadata": {},
   "source": [
    "#### Customer Who Visited but Did Not Make Any Transactions\n",
    "#### https://leetcode.com/problems/customer-who-visited-but-did-not-make-any-transactions/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b815c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_schema = StructType([StructField('visit_id', IntegerType(), False),\n",
    "                            StructField('customer_id', IntegerType(), False)])\n",
    "visits_df = spark.createDataFrame([[1, 23],\n",
    "                                   [2, 9],\n",
    "                                   [4, 30],\n",
    "                                   [5, 54],\n",
    "                                   [6, 96],\n",
    "                                   [7, 54],\n",
    "                                   [8, 54]], schema=visits_schema)\n",
    "\n",
    "transactions_schema = StructType([StructField('transaction_id', IntegerType(), False),\n",
    "                                  StructField(\n",
    "                                      'visit_id', IntegerType(), False),\n",
    "                                  StructField('amount', IntegerType(), False)])\n",
    "transactions_df = spark.createDataFrame([[2, 5, 310],\n",
    "                                         [3, 5, 300],\n",
    "                                         [9, 5, 200],\n",
    "                                         [12, 1, 910],\n",
    "                                         [13, 2, 970]], schema=transactions_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc9394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|visit_id|customer_id|\n",
      "+--------+-----------+\n",
      "|       1|         23|\n",
      "|       2|          9|\n",
      "|       4|         30|\n",
      "|       5|         54|\n",
      "|       6|         96|\n",
      "|       7|         54|\n",
      "|       8|         54|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d62878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------+\n",
      "|transaction_id|visit_id|amount|\n",
      "+--------------+--------+------+\n",
      "|             2|       5|   310|\n",
      "|             3|       5|   300|\n",
      "|             9|       5|   200|\n",
      "|            12|       1|   910|\n",
      "|            13|       2|   970|\n",
      "+--------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5411d75",
   "metadata": {},
   "source": [
    "> If transactions_df is small in size -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835ddd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         30|             1|\n",
      "|         96|             1|\n",
      "|         54|             2|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_visit_ids_who_transacted = [\n",
    "    i[0] for i in transactions_df.select('visit_id').distinct().collect()]\n",
    "visits_df.filter(~visits_df['visit_id'].isin(unique_visit_ids_who_transacted))\\\n",
    "    .groupBy('customer_id').count()\\\n",
    "    .withColumnRenamed('count', 'count_no_trans').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd9021",
   "metadata": {},
   "source": [
    "> If transactions_df is large to run collect(), we can use left join and picking records with null transaction_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6bf03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.join(transactions_df, on='visit_id', how='left')\\\n",
    "    .filter(col('transaction_id').isNull())\\\n",
    "    .groupBy('customer_id').agg(count('visit_id'))\\\n",
    "    .withColumnRenamed('count(visit_id)', 'count_no_trans')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1798a",
   "metadata": {},
   "source": [
    "#### Product Price at a Given Date\n",
    "#### https://leetcode.com/problems/product-price-at-a-given-date/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603fd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_schema = StructType([StructField('product_id', IntegerType(), False),\n",
    "                            StructField('new_price', IntegerType(), True),\n",
    "                            StructField('change_date', StringType(), True)])\n",
    "product_df = spark.createDataFrame([[1, 20, '2019-08-14'],\n",
    "                                    [2, 50, '2019-08-14'],\n",
    "                                    [1, 30, '2019-08-15'],\n",
    "                                    [1, 35, '2019-08-16'],\n",
    "                                    [2, 65, '2019-08-17'],\n",
    "                                    [3, 20, '2019-08-18']], schema=product_schema).select('product_id', 'new_price', col('change_date').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5061824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|product_id|new_price|change_date|\n",
      "+----------+---------+-----------+\n",
      "|         1|       20| 2019-08-14|\n",
      "|         2|       50| 2019-08-14|\n",
      "|         1|       30| 2019-08-15|\n",
      "|         1|       35| 2019-08-16|\n",
      "|         2|       65| 2019-08-17|\n",
      "|         3|       20| 2019-08-18|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee071fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|price|\n",
      "+----------+-----+\n",
      "|         2|   50|\n",
      "|         1|   35|\n",
      "|         3|   10|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min as min_\n",
    "# note - it is important to alias the pyspark min function since otherwise it would use python min() function and break the code\n",
    "\n",
    "product_df.filter(col('change_date') <= '2019-08-16')\\\n",
    "    .select('product_id', first('new_price').over(Window.partitionBy('product_id').orderBy(col('change_date').desc())).alias('price'))\\\n",
    "    .union(\n",
    "    product_df.groupBy('product_id').agg(\n",
    "        min_('change_date').alias('change_date'))\n",
    "    .filter(col('change_date') > '2019-08-16').withColumn('price', lit(10)).select('product_id', 'price')\n",
    ").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207672d",
   "metadata": {},
   "source": [
    "#### Immediate Food Delivery II\n",
    "#### https://leetcode.com/problems/immediate-food-delivery-ii/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2cca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_schema = StructType([StructField('delivery_id', IntegerType(), False),\n",
    "                              StructField('customer_id', IntegerType(), False),\n",
    "                              StructField('order_date', StringType(), True),\n",
    "                              StructField('customer_pref_delivery_date', StringType(), True)])\n",
    "\n",
    "delivery_df = spark.createDataFrame([[1, 1, '2019-08-01', '2019-08-02'],\n",
    "                                     [2, 2, '2019-08-02', '2019-08-02'],\n",
    "                                     [3, 1, '2019-08-11', '2019-08-12'],\n",
    "                                     [4, 3, '2019-08-24', '2019-08-24'],\n",
    "                                     [5, 3, '2019-08-21', '2019-08-22'],\n",
    "                                     [6, 2, '2019-08-11', '2019-08-13'],\n",
    "                                     [7, 4, '2019-08-09', '2019-08-09']], schema=delivery_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a52dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = delivery_df.withColumn('order_date', col('order_date').cast('date'))\\\n",
    "    .withColumn('customer_pref_delivery_date', col('customer_pref_delivery_date').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e93745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+---------------------------+\n",
      "|delivery_id|customer_id|order_date|customer_pref_delivery_date|\n",
      "+-----------+-----------+----------+---------------------------+\n",
      "|          1|          1|2019-08-01|                 2019-08-02|\n",
      "|          2|          2|2019-08-02|                 2019-08-02|\n",
      "|          3|          1|2019-08-11|                 2019-08-12|\n",
      "|          4|          3|2019-08-24|                 2019-08-24|\n",
      "|          5|          3|2019-08-21|                 2019-08-22|\n",
      "|          6|          2|2019-08-11|                 2019-08-13|\n",
      "|          7|          4|2019-08-09|                 2019-08-09|\n",
      "+-----------+-----------+----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab9e6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min as min_\n",
    "immediate_orders = delivery_df.filter(col('order_date') == col(\n",
    "    'customer_pref_delivery_date')).withColumn('immediate', lit(1))\n",
    "first_orders = delivery_df.groupBy('customer_id').agg(\n",
    "    min_('order_date').alias('order_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "677ee5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+---------------------------+---------+\n",
      "|delivery_id|customer_id|order_date|customer_pref_delivery_date|immediate|\n",
      "+-----------+-----------+----------+---------------------------+---------+\n",
      "|          2|          2|2019-08-02|                 2019-08-02|        1|\n",
      "|          4|          3|2019-08-24|                 2019-08-24|        1|\n",
      "|          7|          4|2019-08-09|                 2019-08-09|        1|\n",
      "+-----------+-----------+----------+---------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immediate_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fd03cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|order_date|\n",
      "+-----------+----------+\n",
      "|          1|2019-08-01|\n",
      "|          2|2019-08-02|\n",
      "|          3|2019-08-21|\n",
      "|          4|2019-08-09|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8321c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|                50.0|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_orders.join(immediate_orders,\n",
    "                  how='left',\n",
    "                  on=['customer_id', 'order_date'])\\\n",
    "    .select(round((sum(col('immediate')) / count('*')) * 100, 2).alias('immediate_percentage')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b1e28",
   "metadata": {},
   "source": [
    "#### Department Top Three Salaries\n",
    "#### https://leetcode.com/problems/department-top-three-salaries/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6f4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                             StructField('name', StringType(), False),\n",
    "                             StructField('salary', IntegerType(), True),\n",
    "                             StructField('departmentId', IntegerType(), True)])\n",
    "employee_df = spark.createDataFrame([[1, 'Joe', 85000, 1],\n",
    "                                     [2, 'Henry', 80000, 2],\n",
    "                                     [3, 'Sam', 60000, 2],\n",
    "                                     [4, 'Max', 90000, 1],\n",
    "                                     [5, 'Janet', 69000, 1],\n",
    "                                     [6, 'Randy', 85000, 1],\n",
    "                                     [7, 'Will', 70000, 1]], schema=employee_schema)\n",
    "\n",
    "department_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                                StructField('name', StringType(), True)])\n",
    "department_df = spark.createDataFrame([[1, 'IT'],\n",
    "                                       [2, 'Sales']], schema=department_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2699f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+\n",
      "| id| name|salary|departmentId|\n",
      "+---+-----+------+------------+\n",
      "|  1|  Joe| 85000|           1|\n",
      "|  2|Henry| 80000|           2|\n",
      "|  3|  Sam| 60000|           2|\n",
      "|  4|  Max| 90000|           1|\n",
      "|  5|Janet| 69000|           1|\n",
      "|  6|Randy| 85000|           1|\n",
      "|  7| Will| 70000|           1|\n",
      "+---+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1dd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|   IT|\n",
      "|  2|Sales|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9866cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n",
      "|Department|Employee|Salary|\n",
      "+----------+--------+------+\n",
      "|        IT|    Will| 70000|\n",
      "|        IT|   Randy| 85000|\n",
      "|        IT|     Joe| 85000|\n",
      "|        IT|     Max| 90000|\n",
      "|     Sales|     Sam| 60000|\n",
      "|     Sales|   Henry| 80000|\n",
      "+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.withColumn('rnk', dense_rank().over(Window.partitionBy('departmentId').orderBy(col('Salary').desc())))\\\n",
    "    .select(col('name').alias('Employee'), col('salary').alias('Salary'), col('departmentId'))\\\n",
    "    .filter(col('rnk') <= 3)\\\n",
    "    .join(department_df, department_df.id == employee_df.departmentId)\\\n",
    "    .select(col('name').alias('Department'), 'Employee', 'Salary')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2482c62",
   "metadata": {},
   "source": [
    "#### Fix Names in a Table\n",
    "#### https://leetcode.com/problems/fix-names-in-a-table/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca3f5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = StructType([StructField('user_id', IntegerType(), False),\n",
    "                           StructField('name', StringType(), False)])\n",
    "users_df = spark.createDataFrame([[1, 'aLice'],\n",
    "                                  [2, 'bOB']], schema=users_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b3b19e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id| name|\n",
      "+-------+-----+\n",
      "|      1|aLice|\n",
      "|      2|  bOB|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3096c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id| name|\n",
      "+-------+-----+\n",
      "|      1|Alice|\n",
      "|      2|  Bob|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_fix_udf = udf(lambda x: x[0].upper() + x[1:].lower(), StringType())\n",
    "\n",
    "users_df.withColumn('length', length('name'))\\\n",
    "    .select('user_id', case_fix_udf(col('name')).alias('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc40629",
   "metadata": {},
   "source": [
    "#### 570. Managers with at Least 5 Direct Reports\n",
    "#### https://leetcode.com/problems/managers-with-at-least-5-direct-reports/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cdc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                              StructField('name', StringType(), False),\n",
    "                              StructField('department', StringType(), False),\n",
    "                              StructField('managerId', IntegerType(), True)])\n",
    "employee_df = spark.createDataFrame([[101, 'John', 'A', None],\n",
    "                                     [102, 'Dan',  'A', 101],\n",
    "                                     [103, 'James', 'A', 101],\n",
    "                                     [104, 'Amy',  'A', 101],\n",
    "                                     [105, 'Anne', 'A', 101],\n",
    "                                     [106, 'Ron',  'B', 101]], schema=employee_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4301817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n",
      "| id| name|department|managerId|\n",
      "+---+-----+----------+---------+\n",
      "|101| John|         A|     null|\n",
      "|102|  Dan|         A|      101|\n",
      "|103|James|         A|      101|\n",
      "|104|  Amy|         A|      101|\n",
      "|105| Anne|         A|      101|\n",
      "|106|  Ron|         B|      101|\n",
      "+---+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bad9897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.groupBy('managerId').count()\\\n",
    "    .filter(col('count') >= 5).alias('t')\\\n",
    "    .join(employee_df.alias('e'), col('t.managerId') == col('e.id'))\\\n",
    "    .select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45934b22",
   "metadata": {},
   "source": [
    "#### 620. Not Boring Movies\n",
    "#### https://leetcode.com/problems/not-boring-movies/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ad9728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema_schema = StructType([StructField('id', IntegerType(), False),\n",
    "                            StructField('movie', StringType(), True),\n",
    "                            StructField('description', StringType(), True),\n",
    "                            StructField('rating', FloatType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccc3089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema_df = spark.createDataFrame([[1, 'War', 'great 3D', 8.9],\n",
    "                                   [2, 'Science', 'fiction', 8.5],\n",
    "                                   [3, 'Irish', 'boring', 6.2],\n",
    "                                   [4, 'Ice song', 'Fantacy', 8.6],\n",
    "                                   [5, 'House card', 'Interesting', 9.1]], schema=cinema_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "433fbffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "|  2|   Science|    fiction|   8.5|\n",
      "|  3|     Irish|     boring|   6.2|\n",
      "|  4|  Ice song|    Fantacy|   8.6|\n",
      "|  5|House card|Interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee59b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.filter((col('description') != 'boring') & (col('id') % 2 == 1)).orderBy(col('rating').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995f177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
