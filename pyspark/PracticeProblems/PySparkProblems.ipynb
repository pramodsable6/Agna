{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f2c3b8-3c28-441f-873d-ed019111f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, TimestampType\n",
    "from pyspark.sql.functions import sum, col, count, when, lead, collect_list\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc3c92b-eb15-49f9-8b16-3e3939c81c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/09 19:58:37 WARN Utils: Your hostname, Pramod resolves to a loopback address: 127.0.1.1; using 172.23.91.132 instead (on interface eth0)\n",
      "23/08/09 19:58:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/09 19:58:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[4]').appName('PysparkDataPipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74311c29-9756-4d65-9cc8-3d8e101390fa",
   "metadata": {},
   "source": [
    "#### Creating an empty Df -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb01bdc-c8b3-498d-8a14-4d061a3c1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('column_a', IntegerType(), True),\n",
    "                    StructField('column_b', StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ff5845b-115b-4fd1-9f6e-6e6f69974f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6481333-129d-4bd0-971f-c8689be62dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cc575-9618-4b57-8d2a-b3f76344ea00",
   "metadata": {},
   "source": [
    "#### Append Data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cacccdd-6f39-4dda-9be4-8112a335a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([[1, 'a'], [2, 'b'], [3, 'c'], [None, 'd']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "11a5bb01-1f41-4e26-b7d3-65b3b8a7b330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|    null|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c9b5417-5d78-4e17-be08-7a56427efdfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96316272-0ac4-4c06-a6c0-169901f316a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|    null|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea79d3-673e-4960-a54f-509b021902b7",
   "metadata": {},
   "source": [
    "#### Fill Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2a0842d-16dc-4950-8a55-d0d7836095a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(1, subset=['column_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18caad25-c871-4232-99da-8c9aae9c66f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|       1|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036857b-81ed-4842-8530-7a768e547fa1",
   "metadata": {},
   "source": [
    "#### Replace all 1's with 2's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f94be0cf-d32a-480e-9070-128b3c8d4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       2|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|       2|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df = df.replace(1, 2, subset=['column_a'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcf1a6-5131-446d-a8a0-a71d1167ecce",
   "metadata": {},
   "source": [
    "#### LeetCode SQL problem\n",
    "#### Replace Employee ID With The Unique Identifier\n",
    "https://leetcode.com/problems/replace-employee-id-with-the-unique-identifier/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e660967-6afb-44b4-8948-2fbe9533bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "employees_schema = StructType([StructField('id', IntegerType(), False), StructField('name', StringType(), True)])\n",
    "employees_df = spark.createDataFrame([[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']], schema=employees_schema)\n",
    "\n",
    "employee_uni_schema = StructType([StructField('id', IntegerType(), False), StructField('unique_id', IntegerType(), True)])\n",
    "employee_uni_df = spark.createDataFrame([[3, 1], [11, 2], [90, 3]], schema=employee_uni_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2fc842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|   Alice|\n",
      "|  7|     Bob|\n",
      "| 11|    Meir|\n",
      "| 90| Winston|\n",
      "|  3|Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0105940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|unique_id|\n",
      "+---+---------+\n",
      "|  3|        1|\n",
      "| 11|        2|\n",
      "| 90|        3|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_uni_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c095e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|     null|     Bob|\n",
      "|        2|    Meir|\n",
      "|        1|Jonathan|\n",
      "|        3| Winston|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employee_uni_df, how='left', on=['id']).select('unique_id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca0e3a",
   "metadata": {},
   "source": [
    "#### Leetcode SQL problem - Confirmation Rate\n",
    "https://leetcode.com/problems/confirmation-rate/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f018350",
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_schema = StructType([StructField('user_id', IntegerType(), False),\n",
    "                             StructField('time_stamp', TimestampType(), True)])\n",
    "confirmations_schema = StructType([StructField('user_id', IntegerType(), True),\n",
    "                                   StructField('time_stamp', TimestampType(), True),\n",
    "                                   StructField('action', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ff7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinups_df = spark.createDataFrame([[3, datetime.strptime('2020-03-21 10:16:13', '%Y-%m-%d %H:%M:%S')], [7, datetime.strptime('2020-01-04 13:57:59', '%Y-%m-%d %H:%M:%S')], [\n",
    "                                  2, datetime.strptime('2020-07-29 23:09:44', '%Y-%m-%d %H:%M:%S')], [6, datetime.strptime('2020-12-09 10:39:37', '%Y-%m-%d %H:%M:%S')]], schema=signups_schema)\n",
    "confirmations_df = spark.createDataFrame([[3, datetime.strptime('2021-01-06 03:30:46', '%Y-%m-%d %H:%M:%S'), 'timeout'], [3, datetime.strptime('2021-07-14 14:00:00', '%Y-%m-%d %H:%M:%S'), 'timeout'], [7, datetime.strptime('2021-06-12 11:57:29', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [7, datetime.strptime('2021-06-13 12:58:28', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [\n",
    "                                         7, datetime.strptime('2021-06-14 13:59:27', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [2, datetime.strptime('2021-01-22 00:00:00', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [2, datetime.strptime('2021-02-28 23:59:59', '%Y-%m-%d %H:%M:%S'), 'timeout']], schema=confirmations_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52820080",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = sinups_df.join(confirmations_df, how='left', on=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81f5df31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      3|              0.0|\n",
      "|      7|              1.0|\n",
      "|      2|              0.5|\n",
      "|      6|              0.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.groupBy('user_id').agg((sum(when(result_df.action == 'confirmed', 1).otherwise(0.00))/ count('*')).alias('confirmation_rate')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717588e0",
   "metadata": {},
   "source": [
    "#### Find Users With Valid E-Mails\n",
    "##### https://leetcode.com/problems/find-users-with-valid-e-mails/description/?envType=study-plan-v2&envId=30-days-of-pandas&lang=pythondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb09434",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('user_id', IntegerType(), False), StructField('name', StringType(), True), StructField('mail', StringType(), True)])\n",
    "df = spark.createDataFrame([[1, 'Winston', 'winston@leetcode.com'],\n",
    "                            [2, 'Jonathan', 'jonathanisgreat'],\n",
    "                            [3, 'Annabelle', 'bella-@leetcode.com'],\n",
    "                            [4, 'Sally', 'sally.come@leetcode.com'],\n",
    "                            [5, 'Marwan', 'quarz#2020@leetcode.com'],\n",
    "                            [6, 'David', 'david69@gmail.com'],\n",
    "                            [7, 'Shapiro', '.shapo@leetcode.com']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b6987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      2| Jonathan|     jonathanisgreat|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "|      5|   Marwan|quarz#2020@leetco...|\n",
      "|      6|    David|   david69@gmail.com|\n",
      "|      7|  Shapiro| .shapo@leetcode.com|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d654e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('mail').rlike('^[a-zA-Z][a-zA-Z0-9_.-]*@leetcode\\.com')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4ea04",
   "metadata": {},
   "source": [
    "#### Consecutive Numbers\n",
    "#### https://leetcode.com/problems/consecutive-numbers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "983086c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('id', IntegerType(), False), StructField('num', StringType(), True)])\n",
    "df = spark.createDataFrame([[1, '1'], [2, '1'], [3, '1'], [4, '2'], [5, '1'], [6, '2'], [7, '2']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e634b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d381c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 15:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''with temp as (\n",
    "                select num,\n",
    "                    lead(num, 1) over (order by id) num1,\n",
    "                    lead(num, 2) over (order by id) num2 \n",
    "                from df\n",
    "                )\n",
    "                select distinct num \n",
    "                from temp\n",
    "                where num = num1\n",
    "                and num = num2''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cece067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('num1', lead('num', 1).over(Window.orderBy('id')))\\\n",
    "    .withColumn('num2', lead('num', 2).over(Window.orderBy('id')))\\\n",
    "    .filter((col('num') == col('num1')) & (col('num') == col('num2')))\\\n",
    "    .select('id').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c528fae",
   "metadata": {},
   "source": [
    "### Students and classes example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0721ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('class', IntegerType(), True),\n",
    "                    StructField('student_id', IntegerType(), True),\n",
    "                    StructField('term', IntegerType(), True),\n",
    "                    StructField('subject', StringType(), True),\n",
    "                    StructField('marks', IntegerType(), True)])\n",
    "\n",
    "df = spark.createDataFrame([[2, 1, 1, 'maths', 10], [2, 1, 2, 'maths', 12], [2, 1, 1, 'english', 14], [2, 1, 2, 'english', 12],\n",
    "                            [3, 2, 1, 'maths', 10], [3, 2, 2, 'maths', 12], [3, 2, 1, 'english', 16], [3, 2, 2, 'english', 14]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b4b8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+-------+-----+\n",
      "|class|student_id|term|subject|marks|\n",
      "+-----+----------+----+-------+-----+\n",
      "|    2|         1|   1|  maths|   10|\n",
      "|    2|         1|   2|  maths|   12|\n",
      "|    2|         1|   1|english|   14|\n",
      "|    2|         1|   2|english|   12|\n",
      "|    3|         2|   1|  maths|   10|\n",
      "|    3|         2|   2|  maths|   12|\n",
      "|    3|         2|   1|english|   16|\n",
      "|    3|         2|   2|english|   14|\n",
      "+-----+----------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54aa1c",
   "metadata": {},
   "source": [
    "> Get class 2 students in following format --> class student_id subject term1 term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8360cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy('class', 'student_id', 'subject')\\\n",
    "    .groupBy('class', 'student_id', 'subject')\\\n",
    "    .agg(collect_list('marks')[0].alias('term1'), collect_list('marks')[1].alias('term2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "827e416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+-----+-----+\n",
      "|class|student_id|subject|term1|term2|\n",
      "+-----+----------+-------+-----+-----+\n",
      "|    2|         1|english|   14|   12|\n",
      "|    2|         1|  maths|   10|   12|\n",
      "|    3|         2|english|   16|   14|\n",
      "|    3|         2|  maths|   10|   12|\n",
      "+-----+----------+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1fb6f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+-----+-----+\n",
      "|class|student_id|subject|term1|term2|\n",
      "+-----+----------+-------+-----+-----+\n",
      "|    2|         1|english|   14|   12|\n",
      "|    2|         1|  maths|   10|   12|\n",
      "+-----+----------+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('class') == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0307df",
   "metadata": {},
   "source": [
    "> Get subject-wise aggregated score with 25% weightage to term1 and 75% weightage to term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4170e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[2, 1, 1, 'maths', 10], [2, 1, 2, 'maths', 12], [2, 1, 1, 'english', 14], [2, 1, 2, 'english', 12],\n",
    "                            [3, 2, 1, 'maths', 10], [3, 2, 2, 'maths', 12], [3, 2, 1, 'english', 16], [3, 2, 2, 'english', 14]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a16ac4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+-------+-----+\n",
      "|class|student_id|term|subject|marks|\n",
      "+-----+----------+----+-------+-----+\n",
      "|    2|         1|   1|  maths|   10|\n",
      "|    2|         1|   2|  maths|   12|\n",
      "|    2|         1|   1|english|   14|\n",
      "|    2|         1|   2|english|   12|\n",
      "|    3|         2|   1|  maths|   10|\n",
      "|    3|         2|   2|  maths|   12|\n",
      "|    3|         2|   1|english|   16|\n",
      "|    3|         2|   2|english|   14|\n",
      "+-----+----------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "552d2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "maths_agg = \\\n",
    "    df.filter(col('subject') == 'maths')\\\n",
    "    .orderBy('student_id', 'term')\\\n",
    "    .groupBy('student_id')\\\n",
    "    .agg((collect_list('marks')[0] * 0.25 + collect_list('marks')[1] * 0.75).alias('maths_agg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ec5379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_agg = \\\n",
    "    df.filter(col('subject') == 'english')\\\n",
    "    .orderBy('student_id', 'term')\\\n",
    "    .groupBy('student_id')\\\n",
    "    .agg((collect_list('marks')[0] * 0.25 + collect_list('marks')[1] * 0.75).alias('english_agg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c847409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|student_id|maths_agg|english_agg|\n",
      "+----------+---------+-----------+\n",
      "|         1|     11.5|       12.5|\n",
      "|         2|     11.5|       14.5|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maths_agg.join(english_agg, how='inner', on='student_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb24be7",
   "metadata": {},
   "source": [
    "#### Exchange Seats -\n",
    "#### https://leetcode.com/problems/exchange-seats/description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64a42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "data = '''\n",
    "id,student\n",
    "1,Abbot\n",
    "2,Doris\n",
    "3,Emerson\n",
    "4,Green\n",
    "5,Jeames\n",
    "'''\n",
    "df = spark.createDataFrame(pd.read_csv(io.StringIO(data), header=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1317c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Abbot|\n",
      "|  2|  Doris|\n",
      "|  3|Emerson|\n",
      "|  4|  Green|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "383d983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|Student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/09 17:41:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "max_id = max(df.select('id').collect())[0]\n",
    "df.select(when(col('id') % 2 == 1,\n",
    "               lead('id', 1, max_id).over(Window.orderBy('id')))\n",
    "          .otherwise(col('id') - 1).alias('id'),\n",
    "          'Student').orderBy('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e6135",
   "metadata": {},
   "source": [
    "#### Tree Node\n",
    "#### https://leetcode.com/problems/tree-node/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22009a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('id', IntegerType(), False), StructField('p_id', IntegerType(), True)])\n",
    "df = spark.createDataFrame([[1, None], [2, 1], [3, 1], [4, 2], [5, 2]], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1587b091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|p_id|\n",
      "+---+----+\n",
      "|  1|null|\n",
      "|  2|   1|\n",
      "|  3|   1|\n",
      "|  4|   2|\n",
      "|  5|   2|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9854cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------------------------+\n",
      "| id|CASE WHEN (p_id IS NULL) THEN Root WHEN ((p_id IS NOT NULL) AND (id IN (NULL, 1, 2))) THEN Inner ELSE Leaf END|\n",
      "+---+--------------------------------------------------------------------------------------------------------------+\n",
      "|  1|                                                                                                          Root|\n",
      "|  2|                                                                                                         Inner|\n",
      "|  3|                                                                                                          Leaf|\n",
      "|  4|                                                                                                          Leaf|\n",
      "|  5|                                                                                                          Leaf|\n",
      "+---+--------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_parent_ids = df.select('p_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "df.select('id', when(col('p_id').isNull(), 'Root')\n",
    "          .when(col('p_id').isNotNull() & col('id').isin(distinct_parent_ids), 'Inner')\n",
    "          .otherwise('Leaf')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a7b23",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2be51b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69ed1df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>p_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  p_id\n",
       "0   1   NaN\n",
       "1   2   1.0\n",
       "2   3   1.0\n",
       "3   4   2.0\n",
       "4   5   2.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cad500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pdf['type'] = np.where(pdf['p_id'].isna(),\n",
    "                       'Root',\n",
    "                       np.where(pdf['id'].isin(pdf['p_id'].unique()) & pdf['p_id'].notna(),\n",
    "                                'Inner',\n",
    "                                'Leaf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "686d311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Inner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Leaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Leaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Leaf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   type\n",
       "0   1   Root\n",
       "1   2  Inner\n",
       "2   3   Leaf\n",
       "3   4   Leaf\n",
       "4   5   Leaf"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[['id', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342b00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
