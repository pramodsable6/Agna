{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0f2c3b8-3c28-441f-873d-ed019111f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, TimestampType\n",
    "from pyspark.sql.functions import sum, col, count, when, lead\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc3c92b-eb15-49f9-8b16-3e3939c81c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/03 14:59:53 WARN Utils: Your hostname, Pramod resolves to a loopback address: 127.0.1.1; using 172.23.91.132 instead (on interface eth0)\n",
      "23/08/03 14:59:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/03 15:00:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[4]').appName('PysparkDataPipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74311c29-9756-4d65-9cc8-3d8e101390fa",
   "metadata": {},
   "source": [
    "#### Creating an empty Df -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb01bdc-c8b3-498d-8a14-4d061a3c1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('column_a', IntegerType(), True),\n",
    "                    StructField('column_b', StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ff5845b-115b-4fd1-9f6e-6e6f69974f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6481333-129d-4bd0-971f-c8689be62dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cc575-9618-4b57-8d2a-b3f76344ea00",
   "metadata": {},
   "source": [
    "#### Append Data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cacccdd-6f39-4dda-9be4-8112a335a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([[1, 'a'], [2, 'b'], [3, 'c'], [None, 'd']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "11a5bb01-1f41-4e26-b7d3-65b3b8a7b330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|    null|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c9b5417-5d78-4e17-be08-7a56427efdfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96316272-0ac4-4c06-a6c0-169901f316a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|    null|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea79d3-673e-4960-a54f-509b021902b7",
   "metadata": {},
   "source": [
    "#### Fill Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2a0842d-16dc-4950-8a55-d0d7836095a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(1, subset=['column_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18caad25-c871-4232-99da-8c9aae9c66f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       1|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|       1|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036857b-81ed-4842-8530-7a768e547fa1",
   "metadata": {},
   "source": [
    "#### Replace all 1's with 2's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f94be0cf-d32a-480e-9070-128b3c8d4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_a|column_b|\n",
      "+--------+--------+\n",
      "|       2|       a|\n",
      "|       2|       b|\n",
      "|       3|       c|\n",
      "|       2|       d|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df = df.replace(1, 2, subset=['column_a'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcf1a6-5131-446d-a8a0-a71d1167ecce",
   "metadata": {},
   "source": [
    "#### LeetCode SQL problem\n",
    "#### Replace Employee ID With The Unique Identifier\n",
    "https://leetcode.com/problems/replace-employee-id-with-the-unique-identifier/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e660967-6afb-44b4-8948-2fbe9533bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "employees_schema = StructType([StructField('id', IntegerType(), False), StructField('name', StringType(), True)])\n",
    "employees_df = spark.createDataFrame([[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']], schema=employees_schema)\n",
    "\n",
    "employee_uni_schema = StructType([StructField('id', IntegerType(), False), StructField('unique_id', IntegerType(), True)])\n",
    "employee_uni_df = spark.createDataFrame([[3, 1], [11, 2], [90, 3]], schema=employee_uni_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2fc842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|   Alice|\n",
      "|  7|     Bob|\n",
      "| 11|    Meir|\n",
      "| 90| Winston|\n",
      "|  3|Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0105940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|unique_id|\n",
      "+---+---------+\n",
      "|  3|        1|\n",
      "| 11|        2|\n",
      "| 90|        3|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_uni_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c095e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|     null|     Bob|\n",
      "|        2|    Meir|\n",
      "|        1|Jonathan|\n",
      "|        3| Winston|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employee_uni_df, how='left', on=['id']).select('unique_id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca0e3a",
   "metadata": {},
   "source": [
    "#### Leetcode SQL problem - Confirmation Rate\n",
    "https://leetcode.com/problems/confirmation-rate/description/?envType=study-plan-v2&envId=top-sql-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f018350",
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_schema = StructType([StructField('user_id', IntegerType(), False),\n",
    "                             StructField('time_stamp', TimestampType(), True)])\n",
    "confirmations_schema = StructType([StructField('user_id', IntegerType(), True),\n",
    "                                   StructField('time_stamp', TimestampType(), True),\n",
    "                                   StructField('action', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ff7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinups_df = spark.createDataFrame([[3, datetime.strptime('2020-03-21 10:16:13', '%Y-%m-%d %H:%M:%S')], [7, datetime.strptime('2020-01-04 13:57:59', '%Y-%m-%d %H:%M:%S')], [\n",
    "                                  2, datetime.strptime('2020-07-29 23:09:44', '%Y-%m-%d %H:%M:%S')], [6, datetime.strptime('2020-12-09 10:39:37', '%Y-%m-%d %H:%M:%S')]], schema=signups_schema)\n",
    "confirmations_df = spark.createDataFrame([[3, datetime.strptime('2021-01-06 03:30:46', '%Y-%m-%d %H:%M:%S'), 'timeout'], [3, datetime.strptime('2021-07-14 14:00:00', '%Y-%m-%d %H:%M:%S'), 'timeout'], [7, datetime.strptime('2021-06-12 11:57:29', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [7, datetime.strptime('2021-06-13 12:58:28', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [\n",
    "                                         7, datetime.strptime('2021-06-14 13:59:27', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [2, datetime.strptime('2021-01-22 00:00:00', '%Y-%m-%d %H:%M:%S'), 'confirmed'], [2, datetime.strptime('2021-02-28 23:59:59', '%Y-%m-%d %H:%M:%S'), 'timeout']], schema=confirmations_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52820080",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = sinups_df.join(confirmations_df, how='left', on=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81f5df31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      3|              0.0|\n",
      "|      7|              1.0|\n",
      "|      2|              0.5|\n",
      "|      6|              0.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.groupBy('user_id').agg((sum(when(result_df.action == 'confirmed', 1).otherwise(0.00))/ count('*')).alias('confirmation_rate')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717588e0",
   "metadata": {},
   "source": [
    "#### Find Users With Valid E-Mails\n",
    "##### https://leetcode.com/problems/find-users-with-valid-e-mails/description/?envType=study-plan-v2&envId=30-days-of-pandas&lang=pythondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb09434",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('user_id', IntegerType(), False), StructField('name', StringType(), True), StructField('mail', StringType(), True)])\n",
    "df = spark.createDataFrame([[1, 'Winston', 'winston@leetcode.com'],\n",
    "                            [2, 'Jonathan', 'jonathanisgreat'],\n",
    "                            [3, 'Annabelle', 'bella-@leetcode.com'],\n",
    "                            [4, 'Sally', 'sally.come@leetcode.com'],\n",
    "                            [5, 'Marwan', 'quarz#2020@leetcode.com'],\n",
    "                            [6, 'David', 'david69@gmail.com'],\n",
    "                            [7, 'Shapiro', '.shapo@leetcode.com']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b6987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      2| Jonathan|     jonathanisgreat|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "|      5|   Marwan|quarz#2020@leetco...|\n",
      "|      6|    David|   david69@gmail.com|\n",
      "|      7|  Shapiro| .shapo@leetcode.com|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d654e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('mail').rlike('^[a-zA-Z][a-zA-Z0-9_.-]*@leetcode\\.com')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4ea04",
   "metadata": {},
   "source": [
    "#### Consecutive Numbers\n",
    "#### https://leetcode.com/problems/consecutive-numbers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "983086c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('id', IntegerType(), False), StructField('num', StringType(), True)])\n",
    "df = spark.createDataFrame([[1, '1'], [2, '1'], [3, '1'], [4, '2'], [5, '1'], [6, '2'], [7, '2']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e634b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d381c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 15:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''with temp as (\n",
    "                select num,\n",
    "                    lead(num, 1) over (order by id) num1,\n",
    "                    lead(num, 2) over (order by id) num2 \n",
    "                from df\n",
    "                )\n",
    "                select distinct num \n",
    "                from temp\n",
    "                where num = num1\n",
    "                and num = num2''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cece067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/03 15:56:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('num1', lead('num', 1).over(Window.orderBy('id')))\\\n",
    "    .withColumn('num2', lead('num', 2).over(Window.orderBy('id')))\\\n",
    "    .filter((col('num') == col('num1')) & (col('num') == col('num2')))\\\n",
    "    .select('id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c528fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
